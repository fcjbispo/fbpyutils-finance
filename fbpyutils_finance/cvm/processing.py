import sqlite3
import pandas as pd
import re  # Added import re
from typing import List, Dict, Tuple, Callable, Any

from fbpyutils.debug import debug_info
from .utils import is_nan_or_empty

# Import specific converters if they are defined elsewhere (e.g., in converters.py)
# This assumes converters like as_int, as_float are available in the scope where eval is called
# or are explicitly imported/defined. For safety, explicitly import them if needed.
from .converters import *  # Import all from converters

# --- Funções de Processamento de Dados ---


def get_expression_and_converters(
    mappings: List[Dict[str, Any]],
) -> Tuple[List[str], Dict[str, Callable]]:
    """
    Generates SQL expressions and a dictionary of converter functions based on header mappings.

    Args:
        mappings (List[Dict[str, Any]]): A list of mapping dictionaries for a specific header hash.
                                         Each dict should contain keys like 'Source_Field',
                                         'Target_Field', 'Transformation1', 'Converter', etc.

    Returns:
        Tuple[List[str], Dict[str, Callable]]: A tuple containing:
            - expressions (List[str]): A list of SQL SELECT expressions (e.g., "source_col AS target_col").
            - converters (Dict[str, Callable]): A dictionary mapping target field names (lowercase)
                                                to their corresponding converter functions.
    """
    expressions = []
    converters = {}

    for m in mappings:
        target_field = m.get("Target_Field")
        source_field = m.get("Source_Field")
        converter_str = m.get("Converter")

        # Skip if target field is missing
        if not target_field:
            continue

        target_field_lower = target_field.lower()

        # Build SQL expression
        expression = "NULL"  # Default if no source field
        if not is_nan_or_empty(source_field):
            # Ensure source field name is safe for SQL (basic quoting)
            # A more robust solution might involve checking for actual SQL keywords/injection
            # Assuming source field names from CVM files are generally safe or handled by pandas/sqlite
            safe_source_field = f'"{source_field.lower()}"'  # Use lowercase and quote
            expression = safe_source_field
            # Apply transformations sequentially
            for transform_key in [
                "Transformation1",
                "Transformation2",
                "Transformation3",
            ]:
                transformation = m.get(transform_key)
                if not is_nan_or_empty(transformation):
                    # Replace placeholder $X (case-insensitive) with the current expression
                    # Use regex for safer replacement to avoid partial matches if $X appears within names
                    # expression = transformation.replace('$X', expression) # Simple replacement
                    expression = re.sub(
                        r"\$X", expression, transformation, flags=re.IGNORECASE
                    )

        expressions.append(f"{expression} AS {target_field_lower}")

        # Get converter function
        if not is_nan_or_empty(converter_str):
            try:
                # Evaluate the converter string to get the function object
                # WARNING: eval() is a security risk if converter strings come from untrusted sources.
                # Ensure HEADER_MAPPINGS_FILE is secure and manually curated.
                # Using imported converters module scope for eval.
                converter_func = eval(
                    converter_str, globals(), locals()
                )  # Provide scope
                if not callable(converter_func):
                    raise TypeError(
                        f"Converter string '{converter_str}' did not evaluate to a callable function."
                    )
                converters[target_field_lower] = converter_func
            except Exception as e:
                print(
                    f"Warning: Could not evaluate converter '{converter_str}' for field '{target_field}'. Using default (identity). Error: {e}"
                )
                # Fallback to a function that returns the original value
                converters[target_field_lower] = lambda x: x  # Identity function
        else:
            # If no converter specified, use identity function
            converters[target_field_lower] = lambda x: x  # Identity function

    return expressions, converters


def apply_expressions(data: pd.DataFrame, expressions: List[str]) -> pd.DataFrame:
    """
    Applies SQL expressions to a DataFrame using an in-memory SQLite database.

    Args:
        data (pd.DataFrame): The input DataFrame. Column names should be lowercase.
        expressions (List[str]): A list of SQL SELECT expressions generated by
                                 get_expression_and_converters.

    Returns:
        pd.DataFrame: The resulting DataFrame after applying the expressions.

    Raises:
        ValueError: If applying expressions fails.
        sqlite3.Error: If database operations fail.
    """
    query = ""  # Initialize query string for error reporting
    if data.empty:
        print(
            "Warning: Input DataFrame is empty in apply_expressions. Returning empty DataFrame."
        )
        # Try to determine columns from expressions if possible
        expected_cols = []
        for expr in expressions:
            # Regex to find 'AS alias' part, handling potential spaces and case
            match = re.search(r"AS\s+([\w\"\'\[\`]+)", expr, re.IGNORECASE)
            if match:
                # Clean up potential quotes from alias
                col_name = match.group(1).strip("\"'[]`")
                expected_cols.append(col_name)
        return pd.DataFrame(columns=expected_cols)

    # Ensure column names are lowercase before inserting into SQLite
    data.columns = [c.lower() for c in data.columns]

    STAGE = sqlite3.connect(":memory:")
    try:
        # Use chunking for potentially large DataFrames to manage memory
        chunksize = 100000  # Adjust as needed
        data.to_sql(
            "if_data", con=STAGE, if_exists="replace", index=False, chunksize=chunksize
        )

        query = f"SELECT {', '.join(expressions)} FROM if_data"
        # print(f"Executing SQL: {query}") # Optional: for debugging
        result_df = pd.read_sql(query, con=STAGE)
        return result_df
    except (sqlite3.Error, pd.io.sql.DatabaseError, ValueError) as e:
        info = debug_info(e)
        # Try to provide more context in the error
        raise ValueError(
            f"Failed to apply expressions via SQLite: {e} ({info}). Query: {query[:500]}..."
        )
    finally:
        if STAGE:
            STAGE.close()


def apply_converters(
    data: pd.DataFrame, converters: Dict[str, Callable]
) -> pd.DataFrame:
    """
    Applies converter functions to the columns of a DataFrame.

    Args:
        data (pd.DataFrame): The input DataFrame.
        converters (Dict[str, Callable]): Dictionary mapping column names (lowercase)
                                          to converter functions.

    Returns:
        pd.DataFrame: The DataFrame with converters applied. NaN/NaT values are replaced with None.

    Raises:
        ValueError: If applying a converter fails for a column.
    """
    if data.empty:
        print(
            "Warning: Input DataFrame is empty in apply_converters. Returning empty DataFrame."
        )
        return data

    converted_data = data.copy()  # Work on a copy

    for col_name, converter_func in converters.items():
        if col_name in converted_data.columns:
            original_dtype = converted_data[col_name].dtype
            try:
                # Apply the converter function
                # Use .map for potentially better performance on Series than .apply
                converted_data[col_name] = converted_data[col_name].map(converter_func)
                # Optional: Log type changes for debugging
                # new_dtype = converted_data[col_name].dtype
                # if original_dtype != new_dtype:
                #     print(f"Converted column '{col_name}' from {original_dtype} to {new_dtype}")
            except Exception as e:
                info = debug_info(e)
                # Provide more context about the failing column and converter
                raise ValueError(
                    f"Converter error applying '{getattr(converter_func, '__name__', 'lambda')}' to column '{col_name}': {e} ({info})"
                )
        # else: # Optional: Warn if a converter is defined for a non-existent column
        # print(f"Warning: Converter defined for non-existent column '{col_name}'.")

    # Replace pandas NaNs/NaTs with Python None for database compatibility or general use
    # Using fillna(None) can be tricky with mixed types or non-nullable types.
    # A more robust approach is often to convert columns to object type first if Nones are needed,
    # but this impacts memory and performance. Let's try a careful fillna.
    try:
        # Iterate through columns and fill NA based on dtype
        for col in converted_data.columns:
            # Check if column has NA values to avoid unnecessary operations
            if converted_data[col].isnull().any():
                # Use appropriate NA representation or None
                if pd.api.types.is_integer_dtype(
                    converted_data[col].dtype
                ) and not hasattr(converted_data[col].dtype, "na_value"):
                    # Standard int dtypes don't support None, convert to float or object if NAs exist
                    # Or use pandas nullable Int64Dtype if appropriate upstream
                    # For now, convert to object to allow None
                    converted_data[col] = (
                        converted_data[col].astype(object).fillna(value=None)
                    )
                elif pd.api.types.is_bool_dtype(
                    converted_data[col].dtype
                ) and not hasattr(converted_data[col].dtype, "na_value"):
                    # Standard bool dtype doesn't support None
                    converted_data[col] = (
                        converted_data[col].astype(object).fillna(value=None)
                    )
                else:
                    # For float, datetime, timedelta, string, object, and nullable dtypes, fillna(None) should work
                    # Note: fillna(None) might convert nullable Int to float if None is introduced.
                    # Using pd.NA might be better for consistency if downstream handles it.
                    # Let's stick to None for broader compatibility for now.
                    converted_data[col] = converted_data[col].fillna(value=None)

    except Exception as fill_e:
        print(
            f"Warning: Error during final NA -> None conversion: {fill_e}. Returning data with potential NAs."
        )

    return converted_data
