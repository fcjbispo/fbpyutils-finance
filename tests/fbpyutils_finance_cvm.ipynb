{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c446934",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Providers: CVM Update Package\n",
    "Provides updating data modules, functions and classes for CVM (Comissão de Valores Imobiliários) provider.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ac4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291060a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "from typing import Union, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fbpyutils_finance as FI\n",
    "from fbpyutils_finance.cvm import converters as C\n",
    "from fbpyutils import string as SU, file as FU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8156686",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from fbpyutils.debug import debug, debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b42b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_IF_REGISTER = \"http://dados.cvm.gov.br/dados/FI/CAD/DADOS\"\n",
    "URL_IF_REGISTER_HIST = \"http://dados.cvm.gov.br/dados/FI/CAD/DADOS/HIST\"\n",
    "URL_IF_DAILY = \"http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS\"\n",
    "URL_IF_DAILY_HIST = \"http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/HIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab54423",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_ENCODING, TARGET_ENCODING = 'iso-8859-1', 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_FILE = os.path.sep.join([FI.APP_FOLDER, 'cvm', 'data', 'if_headers_v3.xlsx'])\n",
    "HEADER_MAPPINGS_FILE = os.path.sep.join([FI.APP_FOLDER, 'cvm', 'data', 'if_header_mappings.xlsx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7127e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(HEADERS_FILE):\n",
    "    raise FileNotFoundError('CVM Headers File not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(HEADER_MAPPINGS_FILE):\n",
    "    raise FileNotFoundError('CVM Headers Mappings File not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = pd.read_excel(HEADERS_FILE, sheet_name='IF_HEADERS')\n",
    "HEADER_MAPPINGS = pd.read_excel(HEADER_MAPPINGS_FILE, sheet_name='IF_HEADERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_value_by_index_if_exists(x, y, z=None):\n",
    "    \"\"\"\n",
    "    Returns the value at index y in list x if it exists, otherwise returns z or None.\n",
    "     Args:\n",
    "        x (list): The input list.\n",
    "        y (int): The index to retrieve the value from.\n",
    "        z (optional): The default value to return if the index is out of range. Defaults to None.\n",
    "     Returns:\n",
    "        The value at index y in list x if it exists, otherwise returns z or None.\n",
    "     Overview of General Functionality:\n",
    "        The _get_value_by_index_if_exists function allows retrieving a value from a list at a specified index, with the \n",
    "        flexibility to provide a default value if the index is out of range or the list is empty.\n",
    "    \"\"\"\n",
    "    return x[y] if len(x) > y else z or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_number_type(x, y=int):\n",
    "    \"\"\"\n",
    "    Converts the input x to a specified number type y.\n",
    "     Args:\n",
    "        x (str): The input value to be converted.\n",
    "        y (type, optional): The desired number type to convert to. Defaults to int.\n",
    "     Returns:\n",
    "        The converted value of x to the specified number type y, or None if x is None or '-'.\n",
    "     Overview of General Functionality:\n",
    "        The _make_number_type function is responsible for converting a string representation of a number to a specified number \n",
    "        type. It handles cases where x is None or '-', returning None in those cases. The default number type is int, but it can \n",
    "        be customized by providing a different number type y.\n",
    "    \"\"\"\n",
    "    return None if x is None or x == '-' else y(re.sub(r'[a-zA-Z]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _timelapse(x):\n",
    "    \"\"\"\n",
    "    Calculates the time elapsed in minutes between the current datetime and a given datetime.\n",
    "     Args:\n",
    "        x (datetime): The datetime to calculate the time elapsed from.\n",
    "     Returns:\n",
    "        float: The time elapsed in minutes, rounded to 4 decimal places.\n",
    "     Overview of General Functionality:\n",
    "        The _timelapse function calculates the time elapsed in minutes between the current datetime and a given datetime. It \n",
    "        provides a convenient way to measure the time difference in minutes.\n",
    "    \"\"\"\n",
    "    return round((datetime.now() - x).seconds / 60, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_all(x: str, old: str, new: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all occurrences of `old` with `new` in `x`.\n",
    "\n",
    "    Args:\n",
    "    - x (str): The input string.\n",
    "    - old (str): The string to be replaced.\n",
    "    - new (str): The replacement string.\n",
    "\n",
    "    Returns:\n",
    "    - str: The input string with all occurrences of `old` replaced by `new`.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    x = 'hello, world'\n",
    "    old = 'o'\n",
    "    new = '0'\n",
    "    print(_replace_all(x, old, new))\n",
    "    # Output: 'hell0, w0rld'\n",
    "    ```\n",
    "    \"\"\"\n",
    "    while old in x:\n",
    "        x = x.replace(old, new)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_datetime(x: str, y: str) -> Union[datetime, None]:\n",
    "    \"\"\"\n",
    "    Convert a string representation of a date and time into a datetime object.\n",
    "\n",
    "    Args:\n",
    "    - x (str): The string representation of the date.\n",
    "    - y (str): The string representation of the time.\n",
    "\n",
    "    Returns:\n",
    "    - datetime: The resulting datetime object, or None if either `x` or `y` are None.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    x = '29-Jan-2023'\n",
    "    y = '08:30'\n",
    "    print(_make_datetime(x, y))\n",
    "    # Output: datetime.datetime(2023, 1, 29, 8, 30)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    sep = ' '\n",
    "    if not all([x, y]):\n",
    "        return None\n",
    "    else:\n",
    "        dt = sep.join([x, y])\n",
    "        return datetime.strptime(dt, \"%d-%b-%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f71c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_url_paths(url: str, params: Optional[Dict] = {}) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the URL paths of the given URL with the given parameters.\n",
    "\n",
    "    Args:\n",
    "    - url: URL to get the paths from\n",
    "    - params: query parameters to include in the request, if any (default: {})\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame of the extracted URL paths information, with columns:\n",
    "        - 'sequence': index of the path in the extracted list\n",
    "        - 'href': the URL of the path\n",
    "        - 'name': name of the path\n",
    "        - 'last_modified': last modification date of the path\n",
    "        - 'size': size of the path in bytes\n",
    "    \n",
    "    \"\"\"\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.ok:\n",
    "        response_text = response.text\n",
    "    else:\n",
    "        return response.raise_for_status()\n",
    "    soup = BeautifulSoup(response_text, 'html.parser')\n",
    "    pre = soup.find_all('pre')\n",
    "\n",
    "    pre = pre[0] if len(pre) > 0 else None\n",
    "\n",
    "    sep = pre.text[3:5]\n",
    "\n",
    "    hrefs = [a.get('href') for a in [p for p in pre] if type(a) if type(a) == bs4.element.Tag]\n",
    "\n",
    "    contents = [_replace_all(p, '  ', ' ').split(' ') for p in pre.text.split(sep)]\n",
    "\n",
    "    directory = set()\n",
    "\n",
    "    for i, href in enumerate(hrefs):\n",
    "        directory.add((\n",
    "            i,\n",
    "            href,\n",
    "            _get_value_by_index_if_exists(contents[i], 0),\n",
    "            _make_datetime(\n",
    "                _get_value_by_index_if_exists(contents[i], 1), \n",
    "                _get_value_by_index_if_exists(contents[i], 2)\n",
    "            ),\n",
    "            _make_number_type(_get_value_by_index_if_exists(contents[i], 3))\n",
    "        ))\n",
    "\n",
    "    headers = ['sequence', 'href', 'name', 'last_modified', 'size']\n",
    "    directory = pd.DataFrame(directory, columns=headers).sort_values(by='sequence', ascending=True)\n",
    "\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _build_target_file_name(metadata, target_folder, index=None, file=None):\n",
    "    \"\"\"\n",
    "    Builds the target file name based on the given metadata, target folder, index, and file.\n",
    "    Args:\n",
    "        metadata (dict): The metadata dictionary containing information about the file.\n",
    "        target_folder (str): The path to the target folder where the file will be saved.\n",
    "        index (int, optional): The index number to be included in the file name. Defaults to None.\n",
    "        file (str, optional): The file extension to be included in the file name. Defaults to None.\n",
    "    Returns:\n",
    "        str: The full path to the target file name.\n",
    "    Overview of General Functionality:\n",
    "        The _build_target_file_name function is responsible for constructing the target file name based on the given metadata, \n",
    "        target folder path, index number, and file extension. It provides a flexible way to generate file names for saving files \n",
    "        in a specific folder.\n",
    "    \"\"\"\n",
    "    preffix = metadata['name'].split('.')[0]\n",
    "\n",
    "    if file:\n",
    "        index = str(int('0' if index is None else index)).zfill(4)\n",
    "        target_file_name = '.'.join([preffix, index, file])\n",
    "    else:\n",
    "        target_file_name = metadata['name']\n",
    "\n",
    "    target_file_name = '.'.join([metadata['kind'].lower(), target_file_name])\n",
    "\n",
    "    return os.path.sep.join([target_folder, target_file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ebd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _write_target_file(data, metadata, target_folder, index=None, file=None, encoding=TARGET_ENCODING):\n",
    "    \"\"\"\n",
    "    Writes the provided data to a target file with the specified metadata, target folder, index, file, and encoding.\n",
    "    Args:\n",
    "        data (str): The data to be written to the target file.\n",
    "        metadata (dict): The metadata dictionary containing information about the file.\n",
    "        target_folder (str): The path to the target folder where the file will be saved.\n",
    "        index (int, optional): The index number to be included in the file name. Defaults to None.\n",
    "        file (str, optional): The file extension to be included in the file name. Defaults to None.\n",
    "        encoding (str, optional): The encoding to be used for writing the data to the file. Defaults to TARGET_ENCODING.\n",
    "    Returns:\n",
    "        str: The path to the target file where the data was written.\n",
    "    Overview of General Functionality:\n",
    "        The _write_target_file function is responsible for writing the provided data to a target file with the specified metadata, \n",
    "        target folder, index, file, and encoding. It handles the file writing process and returns the path to the written file.\n",
    "    \"\"\"\n",
    "    target_file = _build_target_file_name(metadata, target_folder, index, file)\n",
    "\n",
    "    with open(target_file, 'wb') as f:\n",
    "        f.write(data.encode(encoding))\n",
    "        f.close()\n",
    "    \n",
    "    return target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _update_cvm_history_file(if_metadata):\n",
    "    \"\"\"\n",
    "    Updates the history file for the given metadata.\n",
    "    Args:\n",
    "        if_metadata (dict): The metadata for the file to update.\n",
    "        history_folder (str, optional): The folder to store the history file. If not provided, the default folder will be used.\n",
    "    Returns:\n",
    "        list: A list containing the result of the update operation.\n",
    "    Overview of General Functionality:\n",
    "        The _update_cvm_history_file function updates the history file for the given metadata. It checks if the file needs to be updated, \n",
    "        downloads the file if necessary, determines the file type, and writes the file to the history folder. The result of the update \n",
    "        operation is returned as a list of status, metadata, and message.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if C.is_nan_or_empty(if_metadata['last_download']) \\\n",
    "        or (if_metadata['last_modified'] > if_metadata['last_download']):\n",
    "\n",
    "        response = request.urlopen(if_metadata['url'])\n",
    "\n",
    "        data = response.read()\n",
    "\n",
    "        mime_type = FU.magic.from_buffer(data)\n",
    "        mime_type = mime_type.split(';')[0]\n",
    "\n",
    "        text_mime_types = ('Non-ISO extended-ASCII text', 'ISO-8859 text', )\n",
    "        zip_mime_types = ('Zip archive data, at least v2.0 to extract', )\n",
    "\n",
    "        if any([a for a in zip_mime_types if a in mime_type]):\n",
    "            zip_file = ZipFile(io.BytesIO(data))\n",
    "            for k, v in enumerate(zip_file.namelist()):\n",
    "                response_data = zip_file.open(v).read().decode(SOURCE_ENCODING)\n",
    "                r = _write_target_file(\n",
    "                    response_data, if_metadata, if_metadata['history_folder'], index=k, file=v, encoding=TARGET_ENCODING)\n",
    "                \n",
    "                result.append(['SUCCESS',if_metadata,f'{r} written from {if_metadata[\"url\"]}'])\n",
    "        elif any([a for a in text_mime_types if a in mime_type]):\n",
    "            response_data = data.decode(SOURCE_ENCODING)\n",
    "\n",
    "            r = _write_target_file(response_data, if_metadata, if_metadata['history_folder'], encoding=TARGET_ENCODING)\n",
    "\n",
    "            result.append(['SUCCESS',if_metadata, f'{r} written from {if_metadata[\"url\"]}'])\n",
    "        else:\n",
    "            result.append(['ERROR',if_metadata,'Unknown mime type:{} for url:{}'.format(mime_type, if_metadata['url'])])\n",
    "    else:\n",
    "        result.append(['SKIP','Already updated data for url:{}'.format(if_metadata['url'])])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e52350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _get_remote_files_list(kind: str, current_url: str, history_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a DataFrame containing information of files in remote locations.\n",
    "\n",
    "    Args:\n",
    "    - kind (str): Kind of files to retrieve.\n",
    "    - current_url (str): URL string of the current location.\n",
    "    - history_url (str): URL string of the history location.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing information of the remote files, including `kind`, `url`, `history`\n",
    "    and other information extracted from the URL paths.\n",
    "    \"\"\"\n",
    "    current_dir = _get_url_paths(current_url)\n",
    "    current_dir['history'] = False\n",
    "    current_dir['url'] = current_url\n",
    "\n",
    "    history_dir = _get_url_paths(history_url)\n",
    "    history_dir['history'] = True\n",
    "    history_dir['url'] = history_url\n",
    "\n",
    "    files_dir = pd.concat([\n",
    "        current_dir[~pd.isnull(current_dir['size'])],\n",
    "        history_dir[~pd.isnull(history_dir['size'])]\n",
    "    ]).copy()\n",
    "\n",
    "    files_dir['kind'] = kind\n",
    "\n",
    "    return files_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _get_expression_and_converters(mappings):\n",
    "    \"\"\"\n",
    "    Extracts expressions and converters from the given mappings.\n",
    "    Args:\n",
    "        mappings (list): A list of dictionaries containing mapping information.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - expressions (list): A list of expressions extracted from the mappings.\n",
    "            - converters (dict): A dictionary of converters extracted from the mappings.\n",
    "    Overview of General Functionality:\n",
    "        The _get_expression_and_converters function is responsible for extracting expressions and converters from the given \n",
    "        mappings. It processes the mappings to generate the necessary expressions and converters for data transformation.\n",
    "    \"\"\"\n",
    "    expressions, converters = [], {}\n",
    "\n",
    "    for m in mappings:\n",
    "        expression = 'NULL'\n",
    "        if not C.is_nan_or_empty(m['Source_Field']):\n",
    "            expression = m['Source_Field']\n",
    "            if not C.is_nan_or_empty(m['Transformation1']):\n",
    "                expression = m['Transformation1'].replace('$X', expression)\n",
    "                if not C.is_nan_or_empty(m['Transformation2']):\n",
    "                    expression = m['Transformation2'].replace('$X', expression)\n",
    "                    if not C.is_nan_or_empty(m['Transformation3']):\n",
    "                        expression = m['Transformation3'].replace('$X', expression)\n",
    "\n",
    "        if not C.is_nan_or_empty(m['Converter']):\n",
    "            converters[m['Target_Field']] = eval(m['Converter'].replace('_as_', 'as_'))\n",
    "        else:\n",
    "            converters[m['Target_Field']] = lambda x: None\n",
    "\n",
    "        expressions.append(f\"{expression.lower()} AS {m['Target_Field'].lower()}\")\n",
    "\n",
    "    return expressions, converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _apply_converters(data, converters):\n",
    "    \"\"\"\n",
    "    Applies converters to the specified columns in the given DataFrame.\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame to apply converters to.\n",
    "        converters (dict): A dictionary of converters to apply.\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with converters applied.\n",
    "    Overview of General Functionality:\n",
    "        The _apply_converters function is responsible for applying converters to the specified columns in the given DataFrame. \n",
    "        It iterates over the converters dictionary and applies the corresponding converter function to each column, modifying \n",
    "        the DataFrame in-place.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for k, v in converters.items():\n",
    "            if k in data.columns:\n",
    "                data[k] = data[k].apply(v)\n",
    "        return data.where(pd.notnull(data), None)\n",
    "    except Exception as E:\n",
    "        info = debug_info(E)\n",
    "        raise ValueError(f\"Conversion error: {E} ({info}) on {k}:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49aea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _apply_expressions(data, expressions):\n",
    "    \"\"\"\n",
    "    Applies expressions to the specified DataFrame using an in-memory SQLite database.\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame to apply expressions to.\n",
    "        expressions (list): A list of expressions to apply.\n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting DataFrame after applying the expressions.\n",
    "    Overview of General Functionality:\n",
    "        The _apply_expressions function applies the specified expressions to the given DataFrame using an in-memory SQLite \n",
    "        database. It stores the DataFrame in the database, executes an SQL query to select the specified expressions, and \n",
    "        returns the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    STAGE=sqlite3.connect(':memory:')\n",
    "    try:\n",
    "        data.to_sql('if_data', con=STAGE, if_exists='replace')\n",
    "\n",
    "        return pd.read_sql(f'SELECT {\", \".join(expressions)} FROM if_data', con=STAGE)\n",
    "    finally:\n",
    "        STAGE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b49b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _get_cvm_file_metadata(cvm_file):\n",
    "    \"\"\"\n",
    "    Analyzes a given cvm_file and retrieves its metadata.\n",
    "    Parameters:\n",
    "    - cvm_file: A string representing the path to the cvm_file.\n",
    "    Returns:\n",
    "    - kind: A string representing the kind of the cvm_file.\n",
    "    - sub_kind: A string representing the sub-kind of the cvm_file.\n",
    "    - line: A string representing the first line of the cvm_file.\n",
    "    - hash_value: A string representing the hash value of the metadata.\n",
    "    Code Overview:\n",
    "    The code reads the first line of a cvm_file and extracts its metadata. It determines the kind\n",
    "    and sub-kind of the file based on its name. The resulting metadata is returned along with a hash\n",
    "    value computed from the metadata.\n",
    "    \"\"\"\n",
    "    with open(cvm_file, 'r') as f:\n",
    "        line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "    file_name_parts = cvm_file.split(os.path.sep)[-1].split('.')\n",
    "    kind = file_name_parts[0].upper()\n",
    "    metadata_file = file_name_parts[-2]\n",
    "    sub_kind = 'CAD_FI' if 'cad_fi' == metadata_file.lower() or metadata_file.lower().startswith('inf_cadastral_fi') \\\n",
    "        else 'DIARIO_FI' if metadata_file.lower().startswith('inf_diario_fi') else metadata_file.upper()\n",
    "    line = line.split('\\n')[0]\n",
    "\n",
    "    return kind, sub_kind, line, SU.hash_string(';'.join([kind, sub_kind, line]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _get_cvm_updated_headers(self, cvm_files: list) -> list[dict]:\n",
    "    \"\"\"\n",
    "    This method retrieves updated headers from given cvm files and returns a list of dictionaries where each dictionary \n",
    "    represents a header mapping.\n",
    "     Parameters:\n",
    "    cvm_files (list): A list of cvm files from which headers need to be retrieved.\n",
    "     Returns:\n",
    "    list[dict]: A list of dictionaries where each dictionary represents a header mapping.\n",
    "     Raises:\n",
    "    FileNotFoundError: If the header mappings file does not exist.\n",
    "    ValueError: If cvm_files is None or not a list, or if both source files and existing mappings are empty.\n",
    "    Exception: If there is any error in getting cvm file metadata or in any step of the function.\n",
    "     Example Usage:\n",
    "    cvm_files = [\"file1\", \"file2\", \"file3\"]\n",
    "    updated_headers = _get_cvm_updated_headers(cvm_files)\n",
    "    \"\"\"\n",
    "    step = 'SETTING UP COMPONENTS'\n",
    "    STAGE=sqlite3.connect(':memory:')\n",
    "    try:\n",
    "        step = 'READING HEADERS MAPPINGS INFO'\n",
    "        if not os.path.exists(HEADER_MAPPINGS_FILE):\n",
    "            raise FileNotFoundError(f\"Header Mappings not found.\")\n",
    "\n",
    "        _ = pd.read_excel(\n",
    "            HEADER_MAPPINGS_FILE, sheet_name='IF_HEADERS'\n",
    "        ).to_sql('cvm_if_headers_stg', con=STAGE, index=False, if_exists='replace')\n",
    "\n",
    "        header_mappings = {}\n",
    "\n",
    "        for header in pd.read_sql(\"\"\"\n",
    "            select distinct \"Header\"\n",
    "            from cvm_if_headers_stg\n",
    "        \"\"\", con=STAGE).to_dict('records'):\n",
    "            mappings = pd.read_sql(\"\"\"\n",
    "            select \"Order\", \"Target_Field\", \"Source_Field\", \"Transformation1\", \"Transformation2\", \"Transformation3\", \"Converter\"\n",
    "                from cvm_if_headers_stg \n",
    "                where \"Header\" = %(header)s\n",
    "                order by \"Order\"\n",
    "            \"\"\", con=STAGE, params={'header': header['Header']}).to_dict('records')\n",
    "            header_mappings[header['Header']] = mappings\n",
    "\n",
    "        step = 'READING IF HISTORY FILES'\n",
    "        if cvm_files is not None and type(cvm_files) == list:\n",
    "            if_source_files = cvm_files\n",
    "        else:\n",
    "            raise ValueError(\"CVM files list empty or invalid.\")\n",
    "\n",
    "        if_source_headers = set()\n",
    "\n",
    "        for if_register_file in if_source_files:\n",
    "            try:\n",
    "                kind, sub_kind, header, header_hash = _get_cvm_file_metadata(if_register_file)\n",
    "            except Exception as e:\n",
    "                print(f'Ouch! {e} on {if_register_file}')\n",
    "                raise e\n",
    "            if_source_headers.add((kind, sub_kind, header, header_hash))\n",
    "\n",
    "        _ = pd.DataFrame(\n",
    "            if_source_headers, columns=['Kind', 'Sub_Kind', 'Header', 'Hash']\n",
    "        ).to_sql('cvm_if_source_headers_stg', con=STAGE, index=False, if_exists='replace')\n",
    "\n",
    "        step = 'READING CURRENT HEADERS INFO'\n",
    "        if os.path.exists(HEADERS_FILE):\n",
    "            mappings = pd.read_excel(HEADERS_FILE, sheet_name='IF_HEADERS').to_dict('records')\n",
    "        else:\n",
    "            mappings = []\n",
    "\n",
    "        existing_mappings = set([m.get('Hash') for m in mappings])\n",
    "\n",
    "        if len(if_source_files) == 0 and len(existing_mappings) == 0:\n",
    "            raise ValueError(\"Mappend Headers and/or History Files Not Found.\")\n",
    "\n",
    "        step = 'COMPUTING NEW HEADERS INFO'\n",
    "        for header_group in pd.read_sql(\"\"\"\n",
    "            select distinct \"Kind\", \"Sub_Kind\"\n",
    "            from cvm_if_source_headers_stg\n",
    "        \"\"\", con=STAGE).to_dict('records'):\n",
    "            kind, sub_kind = header_group['Kind'], header_group['Sub_Kind']\n",
    "            header_mapping = header_mappings[kind]\n",
    "\n",
    "            for source_header in pd.read_sql(\"\"\"\n",
    "                select * \n",
    "                from cvm_if_source_headers_stg\n",
    "                where \"Kind\" = %(kind)s and \"Sub_Kind\" = %(sub_kind)s\n",
    "            \"\"\", con=STAGE, params={'kind': kind, 'sub_kind': sub_kind}).to_dict('records'):\n",
    "                header = source_header['Header']\n",
    "                header_hash = source_header['Hash']\n",
    "\n",
    "                if header_hash not in existing_mappings:\n",
    "                    fields = header.split(';')\n",
    "                    for m in header_mapping[:]:\n",
    "                        found = m['Source_Field'] in fields\n",
    "                        mappings.append({\n",
    "                            'Kind': kind,\n",
    "                            'Sub_Kind': sub_kind,\n",
    "                            'Header': header,\n",
    "                            'Hash': header_hash,\n",
    "                            'Order': int(m['Order']),\n",
    "                            'Target_Field': m['Target_Field'],\n",
    "                            'Source_Field': m['Source_Field'] if found else None,\n",
    "                            'Transformation1': m['Transformation1'] if found else None,\n",
    "                            'Transformation2': m['Transformation2'] if found else None,\n",
    "                            'Transformation3': m['Transformation3'] if found else None,\n",
    "                            'Converter': m['Converter'] if found else None,\n",
    "                            'Is_New': True\n",
    "                        })\n",
    "\n",
    "                        if sub_kind in ['CAD_FI', 'DIARIO_FI']:\n",
    "                            max_order = max([\n",
    "                                h['Order'] for h in header_mapping if h['Source_Field'] is not None])\n",
    "                            source_fields = [\n",
    "                                h['Source_Field'] for h in header_mapping if h['Source_Field'] is not None]\n",
    "                            mapped_fields = [\n",
    "                                m['Source_Field'] for m in mappings if m['Target_Field'] is not None] + [\n",
    "                                    m['Source_Field'] for m in mappings if m['Transformation1'] is not None] + [\n",
    "                                        m['Source_Field'] for m in mappings if m['Transformation2'] is not None] + [\n",
    "                                            m['Source_Field'] for m in mappings if m['Transformation3'] is not None] \n",
    "                            for f in fields:\n",
    "                                if not f in source_fields and not f in mapped_fields:\n",
    "                                    max_order += 1\n",
    "                                    mappings.append({\n",
    "                                        'Kind': kind,\n",
    "                                        'Sub_Kind': sub_kind,\n",
    "                                        'Header': header,\n",
    "                                        'Hash': header_hash,\n",
    "                                        'Order': max_order,\n",
    "                                        'Target_Field': None,\n",
    "                                        'Source_Field': f,\n",
    "                                        'Transformation1': None,\n",
    "                                        'Transformation2': None,\n",
    "                                        'Transformation3': None,\n",
    "                                        'Converter': None,\n",
    "                                        'Is_New': True\n",
    "                                    })\n",
    "\n",
    "        return mappings\n",
    "    except Exception as E:\n",
    "        info = debug_info(E)\n",
    "        raise ValueError('Fail to GET CHANGED HEADERS IN CVM FILES on step {}: {}: ({})'.format(step, E, info))\n",
    "    finally:\n",
    "        STAGE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _check_cvm_headers_changed(cvm_files) -> bool:\n",
    "    \"\"\"\n",
    "    Function: _check_cvm_headers_changed\n",
    "    Description:\n",
    "    This function checks if the headers of CVM files have been changed by comparing the hash of the current headers with the hash of the updated headers. It reads the current headers from an Excel file and stores them in a dictionary. It then calls the function _get_cvm_updated_headers to get the updated headers and compares the hashes of the new headers with the hashes of the existing headers. If there are any changes in the headers, the function returns True, otherwise it returns False.\n",
    "     Parameters:\n",
    "    - cvm_files: A list of CVM files to check for header changes.\n",
    "     Returns:\n",
    "    - bool: True if there are changes in the headers, False otherwise.\n",
    "     Exceptions:\n",
    "    - ValueError: If the function fails to get changed headers in CVM files.\n",
    "     Example:\n",
    "    cvm_files = ['file1.cvm', 'file2.cvm']\n",
    "    headers_changed = _check_cvm_headers_changed(cvm_files)\n",
    "    if headers_changed:\n",
    "        print('Headers have been changed.')\n",
    "    else:\n",
    "        print('Headers have not been changed.')\n",
    "    \"\"\"\n",
    "    step = None\n",
    "    try:\n",
    "        step = 'READING CURRENT HEADERS INFO'\n",
    "        if os.path.exists(HEADERS_FILE):\n",
    "            mappings = pd.read_excel(HEADERS_FILE, sheet_name='IF_HEADERS').to_dict('records')\n",
    "        else:\n",
    "            mappings = []\n",
    "\n",
    "        existing_mappings = set([m.get('Hash') for m in mappings])\n",
    "    \n",
    "        new_mappings = _get_cvm_updated_headers(cvm_files)\n",
    "        \n",
    "        return set(m['Hash'] for m in new_mappings) - existing_mappings\n",
    "    except Exception as E:\n",
    "        info = debug_info(E)\n",
    "        raise ValueError('Fail to GET CHANGED HEADERS IN CVM FILES on step {}: {}: ({})'.format(step, E, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _write_cvm_headers_mappings(mappings, file_path) -> bool:\n",
    "    \"\"\"\n",
    "    Writes the mappings of headers to a file.\n",
    "     Parameters:\n",
    "    mappings (dict): A dictionary containing the mappings of headers.\n",
    "    file_path (str): The path to the file where the mappings will be written.\n",
    "     Returns:\n",
    "    bool: True if the mappings were successfully written, False otherwise.\n",
    "     Functionality Description:\n",
    "    1. Sets up the components for writing the mappings.\n",
    "    2. Writes the new headers mappings information to a temporary SQLite database.\n",
    "    3. Reads the mapped headers from the temporary database.\n",
    "    4. Writes the mapped headers to an Excel file.\n",
    "    5. Returns True if the mappings were successfully written, False otherwise.\n",
    "     Overview:\n",
    "    This function takes a dictionary of mappings and writes them to a specified file location.\n",
    "    It sets up the necessary components, writes the mappings to a temporary database, retrieves the mapped\n",
    "    headers, and finally writes them to an Excel file. The function returns True if the mappings were \n",
    "    successfully written, and False otherwise.\n",
    "    \"\"\"\n",
    "    step = 'SETTING UP COMPONENTS'\n",
    "    STAGE=sqlite3.connect(':memory:')\n",
    "    try:\n",
    "        step = 'WRITING NEW HEADERS MAPPINGS INFO'\n",
    "        if mappings:\n",
    "            _ = pd.DataFrame.from_dict(mappings).to_sql('cvm_if_headers_final_stg', STAGE, index=False, if_exists='replace')\n",
    "\n",
    "            if_headers_final = pd.read_sql(\"\"\"\n",
    "                select distinct \n",
    "                    \"Hash\",\n",
    "                    \"Kind\",\n",
    "                    \"Sub_Kind\",\n",
    "                    \"Order\",\n",
    "                    \"Target_Field\",\n",
    "                    \"Source_Field\",\n",
    "                    \"Transformation1\",\n",
    "                    \"Transformation2\",\n",
    "                    \"Transformation3\",\n",
    "                    \"Converter\",\n",
    "                    \"Is_New\"\n",
    "                from cvm_if_headers_final_stg\n",
    "                order by \"Hash\",\n",
    "                        \"Order\"\n",
    "            \"\"\", con=STAGE)\n",
    "        \n",
    "            if_headers_final.to_excel(\n",
    "                file_path, sheet_name='IF_HEADERS', index=False,  encoding=TARGET_ENCODING, freeze_panes=(1, 0), header=True)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as E:\n",
    "        info = debug_info(E)\n",
    "        raise ValueError('Fail TO WRITE NEW HEADERS on step {}: {}: ({})'.format(step, E, info))\n",
    "    finally:\n",
    "        STAGE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _read_cvm_history_file(source_file, apply_converters=True, check_header=False):\n",
    "    \"\"\"\n",
    "    Reads and processes CVM history data from a source file.\n",
    "    Parameters:\n",
    "    - source_file (str): The path to the source file containing CVM history data.\n",
    "    - apply_converters (bool): Flag indicating whether to apply data type conversions. Default is True.\n",
    "    - check_header (bool): Flag indicating whether to check if the file header has changed. Default is False.\n",
    "    Returns:\n",
    "    - kind (str): The kind of CVM data.\n",
    "    - sub_kind (str): The sub-kind of CVM data.\n",
    "    - cvm_if_data (pandas.DataFrame): The processed CVM data.\n",
    "    - partition_cols (list): List of column names used for partitioning the data.\n",
    "    Raises:\n",
    "    - ValueError: If the file header has changed or if header hash is not found.\n",
    "    - Exception: If the headers mappings file is not found.\n",
    "    - ValueError: If an invalid sub-kind or kind is encountered.\n",
    "    Overview:\n",
    "    This function reads and processes CVM history data from a source file. It performs the following steps:\n",
    "    1. Checks the file header if specified.\n",
    "    2. Retrieves data information from the source file.\n",
    "    3. Reads the current headers mappings from a file.\n",
    "    4. Applies data expressions to the source data.\n",
    "    5. Applies data type conversions if specified.\n",
    "    6. Computes period information based on the kind and sub-kind of the data.\n",
    "    7. Selects the data to return based on partitioning columns.\n",
    "    Note:\n",
    "    - The source file should be in CSV format with ';' delimiter and encoded in the TARGET_ENCODING.\n",
    "    - The headers mappings file should be an Excel file with a sheet named 'IF_HEADERS'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        step = 'CHECK FILE HEADER'\n",
    "        if check_header:\n",
    "            if _check_cvm_headers_changed([source_file]):\n",
    "                raise ValueError('Header changed!!')\n",
    "\n",
    "        step = 'DATA INFO FROM SOURCE FILE'\n",
    "        kind, sub_kind, _, header_hash = _get_cvm_file_metadata(source_file)\n",
    "\n",
    "        if not header_hash:\n",
    "            raise ValueError(\"Header hash not found!\")\n",
    "\n",
    "        step = 'READING CURRENT HEADERS INFO'\n",
    "        if not os.path.exists(HEADERS_FILE):\n",
    "            raise Exception('Headers Mappings file not found!')\n",
    "        \n",
    "        header_mappings = pd.read_excel(HEADERS_FILE, sheet_name='IF_HEADERS')\n",
    "\n",
    "        mappings = header_mappings[header_mappings.Hash == header_hash].to_dict('records')\n",
    "\n",
    "        expressions, data_converters = _get_expression_and_converters(mappings)\n",
    "\n",
    "        step = 'READING DATA FROM SOURCE FILE'\n",
    "        if_data = pd.read_csv(source_file, sep=';', encoding=TARGET_ENCODING, dtype=str, quoting=csv.QUOTE_NONE)\n",
    "        if_data.columns = [c.lower() for c in if_data.columns]\n",
    "\n",
    "        step = 'APPLYING DATA EXPRESSIONS'\n",
    "        cvm_if_data = _apply_expressions(if_data, expressions=expressions, in_memory=True)\n",
    "\n",
    "        if apply_converters:\n",
    "            step = 'APPLYING DATA TYPES CONVERSIONS'\n",
    "            cvm_if_data = _apply_converters(cvm_if_data.copy(), data_converters)\n",
    "\n",
    "        cvm_if_data_cols = list(cvm_if_data.columns)\n",
    "        cvm_if_data['kind'] = kind\n",
    "        cvm_if_data['sub_kind'] = sub_kind\n",
    "\n",
    "        step = 'COMPUTING PERIOD INFO'\n",
    "        if kind == 'IF_POSITION':\n",
    "            cvm_if_data['year'] = cvm_if_data['position_date'].apply(lambda x: datetime.fromisoformat(str(x)).strftime('%Y'))\n",
    "            cvm_if_data['period'] = cvm_if_data['position_date'].apply(lambda x: datetime.fromisoformat(str(x)).strftime('%Y-%m'))\n",
    "        elif kind == 'IF_REGISTER':\n",
    "            if sub_kind == 'CAD_FI':\n",
    "                file_name = source_file.split(os.path.sep)[-1]\n",
    "                date_part = file_name.split('.')[-2].split('_')[-1]\n",
    "\n",
    "                if date_part == 'fi':\n",
    "                    date_part = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                cvm_if_data['year'] = pd.to_datetime(date_part, format='%Y-%m-%d').strftime(\"%Y\")\n",
    "                cvm_if_data['period'] = pd.to_datetime(date_part, format='%Y-%m-%d').strftime(\"%Y-%m\")\n",
    "                cvm_if_data['period_date'] = pd.to_datetime(date_part, format='%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "            elif sub_kind.startswith('CAD_FI_HIST'):\n",
    "                cvm_if_data['year'] = cvm_if_data['start_date'].apply(lambda x: datetime.fromisoformat(str(x)).strftime('%Y'))\n",
    "                cvm_if_data['period'] = cvm_if_data['start_date'].apply(lambda x: datetime.fromisoformat(str(x)).strftime('%Y-%m'))\n",
    "            else:\n",
    "                raise ValueError(f'Invalid Sub Kind: {sub_kind} on {source_file}')\n",
    "        else:\n",
    "            raise ValueError(f'Invalid Kind: {kind} on {source_file}')\n",
    "\n",
    "        step = 'SELECT DATA TO RETURN'\n",
    "        partition_cols = ['kind', 'sub_kind']\n",
    "        for c in ['year', 'period', 'period_date']:\n",
    "            if c in cvm_if_data.columns:\n",
    "                partition_cols += [c]\n",
    "\n",
    "        return kind, sub_kind, cvm_if_data[partition_cols + cvm_if_data_cols], partition_cols\n",
    "    except Exception as E:\n",
    "        info = debug_info(E)\n",
    "        raise ValueError('Fail to get CVM IF HISTORY Data on step {}: {} ({})'.format(step, E, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff80f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @debug\n",
    "def _process_cvm_history_file(cvm_file_info):\n",
    "    \"\"\"\n",
    "    Process the CVM history file.\n",
    "    Args:\n",
    "        cvm_file_info (tuple): A tuple containing three elements - name (str), cvm_file (str), and update_time (datetime).\n",
    "    Returns:\n",
    "        list: A list of lists, where each inner list contains the following elements:\n",
    "            - name (str): The name of the file.\n",
    "            - kind (str): The kind of file.\n",
    "            - sub_kind (str): The sub-kind of file.\n",
    "            - cvm_file (str): The file path.\n",
    "            - update_time (datetime): The update time.\n",
    "            - len(cvm_if_data) (int): The length of cvm_if_data.\n",
    "            - partition_cols (str): The partition columns.\n",
    "            - _timelapse(start_time) (str): The time taken for the process.\n",
    "            - step (str): The current step in the process.\n",
    "            - 'SUCCESS' or 'ERROR' (str): Indicates whether the process was successful or encountered an error.\n",
    "            - Error message (str): Provides additional information in case of an error.\n",
    "    Overview:\n",
    "    This function processes a CVM history file. It initializes parameters and variables, reads the metadata of the CVM file,\n",
    "    connects to an in-memory SQLite database, writes the CVM data to a staging table in the database, appends the result to the result list,\n",
    "    closes the database connection, and finally returns the result list.\n",
    "    Note: If any exception occurs during the process, the code handles it by appending an error message to the result list.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    step = 'SETTING UP COMPONENTS'\n",
    "    STAGE=sqlite3.connect(':memory:')\n",
    "    try:\n",
    "        step = 'INITIALIZING PARAMETERS'\n",
    "        name, cvm_file, update_time = cvm_file_info\n",
    "\n",
    "        step = 'READING CVM FILE METADATA'\n",
    "        kind, sub_kind, cvm_if_data, partition_cols = _read_cvm_history_file(\n",
    "            source_file=cvm_file, \n",
    "            apply_converters=True,\n",
    "            check_header=False\n",
    "        )\n",
    "        try:\n",
    "            target_table = f'cvm_{kind}_{sub_kind}_history_stg'.lower()\n",
    "\n",
    "            cvm_if_data['source'] = '.'.join(cvm_file.split(os.path.sep)[-1].split('.')[0:-1])\n",
    "            cvm_if_data['timestamp'] = update_time.isoformat()\n",
    "\n",
    "            step = 'WRITING CVM DATA STAGE'\n",
    "            cvm_if_data.to_sql(target_table, index=False, if_exists='append', con=STAGE)\n",
    "\n",
    "            result.append([\n",
    "                name, kind, sub_kind, cvm_file, update_time, len(cvm_if_data), partition_cols, \n",
    "                _timelapse(start_time), step, 'SUCCESS', None\n",
    "            ])\n",
    "        except Exception as E:\n",
    "            result.append([\n",
    "                name, kind, sub_kind, cvm_file, update_time, len(cvm_if_data), partition_cols, \n",
    "                _timelapse(start_time), step, 'ERROR', f'ERROR: {E}'\n",
    "            ])\n",
    "    except Exception as E:\n",
    "        info = debug_info(E)\n",
    "        if len(cvm_file_info) == 3:\n",
    "            name, cvm_file, update_time = cvm_file_info\n",
    "        else:\n",
    "            name, cvm_file, update_time = None, None, None\n",
    "        result.append([\n",
    "            name, None, None, cvm_file, update_time, None, None, \n",
    "            _timelapse(start_time), step, 'ERROR', f'ERROR {E} ({info}): WITH info={cvm_file_info}'\n",
    "        ])\n",
    "    finally:\n",
    "        STAGE = None\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVM():\n",
    "    @staticmethod\n",
    "    def check_history_folder(history_folder=None):\n",
    "        \"\"\"\n",
    "        Checks if the history folder exists and creates it if it doesn't.\n",
    "            Args:\n",
    "            history_folder (str, optional): The path to the history folder. Defaults to None.\n",
    "            Returns:\n",
    "            str: The path to the history folder.\n",
    "            Return Value or Values and their Purpose:\n",
    "            - history_folder (str): The path to the history folder. This is the same as the input history_folder parameter if provided,\n",
    "                otherwise it is the default path to the history folder.\n",
    "            Overview of General Functionality:\n",
    "            The _check_history_folder function is responsible for ensuring the existence of the history folder. It allows the \n",
    "            flexibility to provide a custom path or use a default path. If the history folder doesn't exist, it creates it.\n",
    "        \"\"\"\n",
    "        history_folder = history_folder or os.path.sep.join([FI.USER_APP_FOLDER, 'history'])\n",
    "        if not os.path.exists(history_folder):\n",
    "            os.makedirs(history_folder)\n",
    "        \n",
    "        return history_folder\n",
    "\n",
    "    def __init__(self, catalog=None, history_folder=None):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the class.\n",
    "        Args:\n",
    "            catalog (Optional[sqlite3.Connection]): A database connection to a catalog.\n",
    "                Defaults to None.\n",
    "            history_folder (Optional[str]): The path to a history folder.\n",
    "                Defaults to None.\n",
    "        Returns:\n",
    "            None\n",
    "        Code Overview:\n",
    "        The code initializes the attributes of the class by either creating new database connections\n",
    "        or assigning the provided connections. It also sets up the CATALOG_JOURNAL attribute with a specific string value.\n",
    "        The purpose of this initialization is to prepare the class for further operations and interactions\n",
    "        with databases and folders.\n",
    "        \"\"\"\n",
    "        if catalog is None or not FI.is_valid_db_connection(catalog):\n",
    "            self.CATALOG = sqlite3.connect(os.path.sep.join([FI.USER_APP_FOLDER, 'catalog.db']))\n",
    "        else:\n",
    "            self.CATALOG = catalog\n",
    "\n",
    "        self.HISTORY_FOLDER = CVM.check_history_folder(history_folder)\n",
    "        \n",
    "        self.CATALOG_JOURNAL = 'cvm_if_catalog_journal'\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Performs cleanup actions when the object is about to be destroyed.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            None\n",
    "        Functionality Description:\n",
    "            - Attempts to close the database connection stored in the CATALOG attribute.\n",
    "            - If an exception occurs during the closing operation, it is ignored.\n",
    "            - Sets the CATALOG attribute to None, indicating that the database connection is no longer valid.\n",
    "        Code Overview:\n",
    "        The code defines a destructor method that is automatically called when the object is about to be destroyed.\n",
    "        It is responsible for closing the database connection stored in the CATALOG attribute and setting the attribute to None.\n",
    "        The purpose of this cleanup is to ensure that resources are properly released and avoid potential memory leaks.\n",
    "        \"\"\"\n",
    "        if FI.is_valid_db_connection(self.CATALOG):\n",
    "            self.CATALOG.close()\n",
    "        self.CATALOG = None\n",
    "\n",
    "    \n",
    "    # @debug\n",
    "    def update_cvm_catalog(self, parallelize=True):\n",
    "        \"\"\"\n",
    "        Updates the history files in the CVM (Comissão de Valores Mobiliários) catalog.\n",
    "        Args:\n",
    "            parallelize (bool, optional): A flag to indicate whether to use parallel processing. If True and the number\n",
    "            of CPUs is more than 1, parallel processing is used. Defaults to True.\n",
    "        Returns:\n",
    "            list: A list of results from the update process. Each result is a tuple containing the name, kind, status,\n",
    "            and message of the processed file.\n",
    "        Overview:\n",
    "        This function updates the history files in the CVM catalog. It sets up components, sets up the catalog journal,\n",
    "        and gets the list of remote files. It then updates the catalog journal with the metadata of the remote files, and\n",
    "        processes the files. If the 'parallelize' argument is True and the number of CPUs is more than 1, it uses parallel\n",
    "        processing. After processing the files, it consolidates the information and updates the CVM catalog journal.\n",
    "        Note: If an exception occurs during the process, it raises a ValueError with the step at which the\n",
    "        error occurred and the error message.\n",
    "        \"\"\"\n",
    "        PARALLELIZE = parallelize and os.cpu_count()>1\n",
    "\n",
    "        # disabling parallel processing due to errors with sqlite3\n",
    "        PARALLELIZE = False\n",
    "\n",
    "        step = 'SETTING UP COMPONENTS'\n",
    "        STAGE = sqlite3.connect(':memory:')\n",
    "        try:\n",
    "            step = \"SETTING UP CATALOG JOURNAL\"\n",
    "\n",
    "            if_register_files = self._get_remote_files_list(\n",
    "                'IF_REGISTER', URL_IF_REGISTER, URL_IF_REGISTER_HIST\n",
    "            )\n",
    "\n",
    "            if_position_files =self._get_remote_files_list(\n",
    "                'IF_POSITION', URL_IF_DAILY, URL_IF_DAILY_HIST\n",
    "            )\n",
    "\n",
    "            if_remote_files = pd.concat([\n",
    "                if_register_files,\n",
    "                if_position_files\n",
    "            ]).copy()\n",
    "\n",
    "            catalog_journal_df = self.get_cvm_catalog_data()\n",
    "            \n",
    "            if type(catalog_journal_df) == pd.DataFrame:\n",
    "                catalog_journal = catalog_journal_df.to_dict('records')\n",
    "            else:\n",
    "                catalog_journal = []\n",
    "\n",
    "            for if_metadata in [f for f in if_remote_files.to_dict('records')]:\n",
    "                if_metadata['url'] = '/'.join([if_metadata['url'], if_metadata['name']])\n",
    "\n",
    "                catalog_metadata = next((m for m in catalog_journal if m['url'] == if_metadata['url']), None)\n",
    "                \n",
    "                if catalog_metadata is None:\n",
    "                    catalog_metadata = if_metadata\n",
    "                    catalog_metadata['history'] = (if_metadata['name'] != 'cad_fi.csv')\n",
    "                    catalog_metadata['last_download'] = None\n",
    "                    catalog_metadata['last_updated'] = None\n",
    "                    catalog_metadata['process'] = True\n",
    "                    catalog_metadata['active'] = True\n",
    "                    catalog_journal.append(catalog_metadata)\n",
    "                else:\n",
    "                    catalog_metadata['process'] = False\n",
    "                    catalog_metadata['history'] = (catalog_metadata['history'] == True or if_metadata['name'] != 'cad_fi.csv')\n",
    "                    if C.is_nan_or_empty(catalog_metadata['last_modified']) \\\n",
    "                        or if_metadata['last_modified'] > catalog_metadata['last_modified']:\n",
    "                        catalog_metadata['last_modified'] = if_metadata['last_modified']\n",
    "                        catalog_metadata['process'] = True\n",
    "                    elif C.is_nan_or_empty(catalog_metadata['last_updated']) \\\n",
    "                        or if_metadata['last_modified'] > catalog_metadata['last_updated']:\n",
    "                        catalog_metadata['process'] = True\n",
    "\n",
    "            catalog_journal_df = pd.DataFrame.from_dict(catalog_journal, orient='columns')\n",
    "\n",
    "            for c in ['last_modified', 'last_download', 'last_updated']:\n",
    "                catalog_journal_df[c] = pd.to_datetime(c, errors='coerce')\n",
    "\n",
    "            catalog_journal_df.to_sql(self.CATALOG_JOURNAL, con=self.CATALOG, if_exists='replace', index=False)\n",
    "\n",
    "            results = []\n",
    "            metadata_to_process = []\n",
    "            for m in catalog_journal:\n",
    "                if m['process']:\n",
    "                    m['history_folder'] = self.HISTORY_FOLDER\n",
    "                    metadata_to_process.append(m)\n",
    "\n",
    "            if PARALLELIZE:\n",
    "                with Pool(os.cpu_count()) as p:\n",
    "                    results = p.map(_update_cvm_history_file, metadata_to_process)\n",
    "            else:\n",
    "                for if_metadata in metadata_to_process:\n",
    "                    result = _update_cvm_history_file(if_metadata)\n",
    "                    results.append(result)\n",
    "\n",
    "            step = 'UPDATE CVM CATALOG JOURNAL: CONSOLIDATING INFO'\n",
    "            result_cols = [\n",
    "                'name', 'kind', 'status', 'message'\n",
    "            ]\n",
    "            result_data = pd.DataFrame(\n",
    "                [(r[0][1]['name'], r[0][1]['kind'], r[0][0], r[0][-1]) for r in results if r[0][0] != 'SKIP'], columns=result_cols\n",
    "            )\n",
    "            result_table = 'cvm_update_history_file_results_stg'\n",
    "            result_data.to_sql(result_table, con=STAGE, index=False, if_exists='replace')\n",
    "\n",
    "            download_time = datetime.now().isoformat()\n",
    "\n",
    "            step = 'UPDATE CVM CATALOG JOURNAL: UPDATING DATA'\n",
    "            for name, kind in pd.read_sql(f'''\n",
    "                with t as (\n",
    "                    select name, kind,\n",
    "                            sum(case when status='ERROR' then 1 else 0 end) errors, \n",
    "                            sum(case when status='SUCCESS' then 1 else 0 end) successes, \n",
    "                            sum(case when status='SKIP' then 1 else 0 end) skips\n",
    "                        from {result_table}\n",
    "                        group by name, kind\n",
    "                )\n",
    "                select name, kind\n",
    "                    from t\n",
    "                    where errors = 0\n",
    "                    and (successes > 0 or skips > 0)\n",
    "            ''', con=STAGE).to_records(index=False):\n",
    "                _ = self.CATALOG.execute(f'''\n",
    "                    update {self.CATALOG_JOURNAL}\n",
    "                        set last_download = '{download_time}',\n",
    "                            process = TRUE\n",
    "                        where name = '{name}'\n",
    "                        and kind = '{kind}';\n",
    "                ''')\n",
    "                sleep(0.3)\n",
    "\n",
    "            return results\n",
    "        except Exception as E:\n",
    "            info = debug_info(E)\n",
    "            raise ValueError('Fail to UPDATE history files on step {}: {} ({})'.format(step, E, info))\n",
    "        finally:\n",
    "            STAGE = None\n",
    "    \n",
    "\n",
    "    # @debug\n",
    "    def get_cvm_catalog(self):\n",
    "        \"\"\"\n",
    "        Retrieves catalog data from a SQL database.\n",
    "        Returns:\n",
    "        - DataFrame: A pandas DataFrame containing the catalog data.\n",
    "        Raises:\n",
    "        - Exception: If there is an error retrieving the catalog data.\n",
    "        Dependencies:\n",
    "        - pandas\n",
    "        Example Usage:\n",
    "          catalog_data = get_cvm_catalog_data()\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.read_sql(f\"\"\"\n",
    "                select *\n",
    "                from {self.CATALOG_JOURNAL}\n",
    "            \"\"\", con=self.CATALOG)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "    # @debug\n",
    "    # def update_cvm_history_data(self, parallelize=True):\n",
    "    #     \"\"\"\n",
    "    #     Updates the history data of Comissão de Valores Mobiliários (CVM) files.\n",
    "    #     Parameters:\n",
    "    #     - parallelize (bool, optional): A boolean flag to indicate if the function should be parallelized. Defaults to True.\n",
    "    #     - history_folder (str, optional): The path to the folder that contains the history data. Defaults to None.\n",
    "    #     Returns:\n",
    "    #     - results (list): A list of dictionaries containing the result data of the CVM files processing.\n",
    "    #     Code Overview:\n",
    "    #     The code updates the history data of CVM files. It checks the validity of the history_folder and the possibility\n",
    "    #     of parallel execution. It then connects to an in-memory SQLite database and tries to execute several steps to\n",
    "    #     update the history data. If an error occurs during the execution of the steps, it raises a ValueError with an\n",
    "    #     appropriate error message. Finally, it closes the connection to the SQLite database.\n",
    "    #     \"\"\"\n",
    "    #     PARALLELIZE = parallelize and os.cpu_count()>1\n",
    "\n",
    "    #     # disabling parallel processing due to errors with sqlite3\n",
    "    #     PARALLELIZE = False\n",
    "\n",
    "    #     step = 'SETUP COMPONENTS'\n",
    "    #     STAGE = sqlite3.connect(':memory:')\n",
    "    #     try:\n",
    "    #         step = 'LOAD CATALOG UPDATES'\n",
    "    #         catalog_updates = pd.read_sql(f'''\n",
    "    #             select * \n",
    "    #             from {self.CATALOG_JOURNAL} \n",
    "    #             where active \n",
    "    #             and last_updated is null \n",
    "    #                 or last_download > last_updated\n",
    "    #             order by kind, name\n",
    "    #         ''', con=self.CATALOG).to_dict(orient='records')\n",
    "\n",
    "    #         results = []\n",
    "    #         if len(catalog_updates) > 0:\n",
    "    #             step = 'SELECTING FILES TO UPDATE'\n",
    "    #             cvm_files = []\n",
    "    #             for name, mask in [\n",
    "    #                 (u['name'], f\"{u['kind'].lower()}.{u['name'].split('.')[0]}.*\") for u in catalog_updates\n",
    "    #             ]:\n",
    "    #                 for cvm_file in FU.find(self.HISTORY_FOLDER, mask):\n",
    "    #                     cvm_files += [[name, cvm_file]]\n",
    "\n",
    "    #             if len(cvm_files) == 0:\n",
    "    #                 return []\n",
    "\n",
    "    #             step = 'CHECKING HEADERS'\n",
    "    #             if self._check_cvm_headers_changed(cvm_files=[f[1] for f in cvm_files]):\n",
    "    #                 raise ValueError('Headers Changed! Update not possible.')\n",
    "\n",
    "    #             step = 'UPDATE CVM HISTORY DATA'\n",
    "    #             update_time = datetime.now()\n",
    "\n",
    "    #             cvm_files = [[f[0], f[1], update_time] for f in cvm_files]\n",
    "\n",
    "    #             if PARALLELIZE:\n",
    "    #                 with Pool(os.cpu_count()) as p:\n",
    "    #                     results = p.map(self._process_cvm_history_file, cvm_files)\n",
    "    #             else:\n",
    "    #                 for cvm_file in cvm_files:\n",
    "    #                     result = self._process_cvm_history_file(cvm_file)\n",
    "    #                     results.append(result)\n",
    "\n",
    "    #             step = 'UPDATE CVM CATALOG JOURNAL: CONSOLIDATING INFO'\n",
    "    #             result_cols = [\n",
    "    #                 'name', 'kind', 'sub_kind', 'cvm_file', 'update_time', 'records', \n",
    "    #                 'partition_cols', 'timelapse', 'last_step', 'status', 'message'\n",
    "    #             ]\n",
    "    #             result_data = pd.DataFrame(\n",
    "    #                 [r for r in [r[0] for r in results]], columns=result_cols\n",
    "    #             )\n",
    "    #             result_table = 'cvm_update_history_results_stg'\n",
    "    #             result_data.to_sql(result_table, con=STAGE, index=False, if_exists='replace')\n",
    "\n",
    "    #             step = 'UPDATE CVM CATALOG JOURNAL: UPDATING DATA'\n",
    "    #             for name, kind, update_time in pd.read_sql(f'''\n",
    "    #                 with t as (\n",
    "    #                     select name, kind,\n",
    "    #                         sum(case when status='ERROR' then 1 else 0 end) errors, \n",
    "    #                         sum(case when status='SUCCESS' then 1 else 0 end) successes,\n",
    "    #                         max(update_time) update_time\n",
    "    #                     from {result_table}\n",
    "    #                     group by name, kind\n",
    "    #                 )\n",
    "    #                 select name, kind, update_time\n",
    "    #                 from t\n",
    "    #                 where errors = 0\n",
    "    #                 and successes > 0\n",
    "    #             ''', con=STAGE).to_records(index=False):\n",
    "    #                 _ = self.CATALOG.execute(f'''\n",
    "    #                     update {self.CATALOG_JOURNAL}\n",
    "    #                     set last_updated = '{update_time}',\n",
    "    #                         process = FALSE\n",
    "    #                     where name = '{name}'\n",
    "    #                     and kind = '{kind}';\n",
    "    #                 ''')\n",
    "    #                 sleep(0.5)\n",
    "\n",
    "    #         return results\n",
    "    #     except Exception as E:\n",
    "    #         info = debug_info(E)\n",
    "    #         raise ValueError('Fail to get CVM UPDATE HISTORY DATA Data on step {}: {} ({}) (name={}, file={})'.format(\n",
    "    #             step, E, info, name, cvm_file))\n",
    "    #     finally:\n",
    "    #         STAGE = None"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fbpyutils-finance--TrezB8H",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
